import streamlit as st
import pandas as pd
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import re
import time
import random
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.callbacks import get_openai_callback
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

@st.cache_data
def load_company_data():
    df = pd.read_csv('company_codes.csv', dtype=str)
    return df

# ê²€ìƒ‰ ì˜µì…˜ ìƒì„±
def create_search_options(df):
    options = [f"{row['íšŒì‚¬ëª…']} ({row['ì¢…ëª©ì½”ë“œ']})" for _, row in df.iterrows()]
    return options

# ì„ íƒëœ ì˜µì…˜ì—ì„œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
def extract_code_from_option(option):
    return option.split('(')[-1].replace(')', '')

# ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
class NewsCrawler:
    def __init__(self, company_data):
        self.company_code_table = company_data
    
    def crawler(self, company_code, num_article):
        session = HTMLSession()
        done_page_num = 0
        num_per_page = 20
        num_page, remainder = divmod(num_article, num_per_page)
        num_page += 1
        article_result = []
        
        for page in range(done_page_num + 1, done_page_num + num_page + 1):
            try:
                url = f'https://finance.naver.com/item/news_news.nhn?code={company_code}&page={page}'
                source_code = session.get(url).text
                html = BeautifulSoup(source_code, "lxml")
                
                links = html.select('.title')
                link_result = []
                if page == num_page:
                    links = links[:remainder]
                for link in links:
                    add = 'https://finance.naver.com' + link.find('a')['href']
                    link_result.append(add)
            except Exception:
                pass
            
            for article_url in link_result:
                try:
                    article_source_code = session.get(article_url).html
                    redirect_url = article_source_code.search("top.location.href='{}';")[0]
                    article_source_code = session.get(redirect_url).text
                    article_html = BeautifulSoup(article_source_code, "lxml")
                    article_time = article_html.select('.media_end_head_info_datestamp_time')[0].get_text()
                    article_title = article_html.select('.media_end_head_title')[0].get_text().strip()
                    article_contents = article_html.select('._article_content')[0].get_text().strip()
                    article_contents = re.sub('\n', '', article_contents)
                    article_contents = re.sub('\t', '', article_contents)
                    
                    if "â“’" in article_contents:
                        article_contents = article_contents[:article_contents.index("â“’")]
                    
                    if len(article_contents) >= 1500:
                        article_contents = article_contents[:1500]

                    article_result.append([article_title, article_contents, article_time])
                    time.sleep(random.uniform(0.1, 0.7))
                except Exception:
                    pass

        return article_result
    
    def convert_company_to_code(self, company):
        if company in self.company_code_table['ì¢…ëª©ì½”ë“œ'].values:
            return company
        elif company in self.company_code_table['íšŒì‚¬ëª…'].values:
            return self.company_code_table[self.company_code_table['íšŒì‚¬ëª…'] == company]['ì¢…ëª©ì½”ë“œ'].values[0]
        else:
            return None
    
    def crawl_news(self, company, max_num=5):
        company_code = self.convert_company_to_code(company)

        if company_code:
            result = self.crawler(company_code, max_num)
            for i in range(len(result)):
                result[i].append(company)
            return result
        else:
            return []

# ë‰´ìŠ¤ ë¶„ì„ í•¨ìˆ˜
def analyze_news(news_data):
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)
    
    template = """
    ë‹¤ìŒì€ {company}ì— ê´€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤:

    {news_content}

    ì´ ë‰´ìŠ¤ë¥¼ ìš”ì•½í•˜ê³ , ì „ë°˜ì ì¸ ë¶„ìœ„ê¸°ê°€ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”. 
    ë˜í•œ í•´ë‹¹ ë¶„ìœ„ê¸°ì˜ ê°•ë„ë¥¼ ë‚˜íƒ€ë‚´ì„¸ìš”. (ìˆ«ì 1~10)
    ê°•ë„ëŠ” í•´ë‹¹ ë‰´ìŠ¤ì˜ ë¶„ìœ„ê¸°ê°€ ì£¼ì‹ ì‹œì¥ì— ì–´ëŠ ì •ë„ì˜ ê°•ë„ë¡œ ì ìš©ë ì§€ë¥¼ ëœ»í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:

    ìš”ì•½: [ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½]
    ë¶„ìœ„ê¸°: [ê¸ì •/ë¶€ì •/ì¤‘ë¦½] [ê°•ë„] 
    (ì´ìœ  ì„¤ëª…)

    í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    """

    prompt = PromptTemplate(
        input_variables=["company", "news_content"],
        template=template,
    )

    chain = LLMChain(llm=llm, prompt=prompt)

    full_content = "\n\n".join([f"ì œëª©: {news[0]}\në‚´ìš©: {news[1]}" for news in news_data])
    company = news_data[0][3]

    with get_openai_callback() as cb:
        result = chain.run(company=company, news_content=full_content)
    
    return result, cb

# Streamlit ì•±
def main():
    st.set_page_config(page_title="ì£¼ì‹ ë‰´ìŠ¤ ë¶„ì„ê¸°", page_icon="ğŸ“ˆ", layout="wide")
    st.title("ğŸ“Š ì£¼ì‹ ë‰´ìŠ¤ ë¶„ì„ê¸°")

    # CSSë¥¼ ì‚¬ìš©í•˜ì—¬ UI ê°œì„ 
    st.markdown("""
    <style>
    .big-font {
        font-size:20px !important;
        font-weight: bold;
    }
    .result-box {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
        color: #222;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ì¢…ëª© ë°ì´í„° ë¡œë“œ
    company_data = load_company_data()
    search_options = create_search_options(company_data)

    # ì‚¬ì´ë“œë°”ì— ì…ë ¥ í¼ ë°°ì¹˜
    with st.sidebar:
        st.header("ğŸ“ ë¶„ì„ ì„¤ì •")
        
        selected_option = st.selectbox(
            "ì¢…ëª©ëª… ë˜ëŠ” ì¢…ëª© ì½”ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”",
            options=search_options
        )
        company_code = extract_code_from_option(selected_option) if selected_option else None
        
        max_news = st.slider("ë¶„ì„í•  ë‰´ìŠ¤ ê°œìˆ˜", 1, 20, 5)
        analyze_button = st.button("ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°")

    if analyze_button and company_code:
        with st.spinner("ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            # ë‰´ìŠ¤ í¬ë¡¤ë§
            crawler = NewsCrawler(company_data)
            news_data = crawler.crawl_news(company_code, max_news)

            if news_data:
                # ë‰´ìŠ¤ ë¶„ì„
                analysis_result, callback = analyze_news(news_data)

                # ê²°ê³¼ í‘œì‹œ
                st.markdown("<p class='big-font'>ë¶„ì„ ê²°ê³¼</p>", unsafe_allow_html=True)
                st.markdown("<div class='result-box'>" + analysis_result.replace("\n", "<br>") + "</div>", unsafe_allow_html=True)

                st.markdown("<p class='big-font'>ì‚¬ìš© ì •ë³´</p>", unsafe_allow_html=True)
                st.markdown(f"""
                <div class='result-box'>
                ì´ ì‚¬ìš©ëœ í† í° ìˆ˜: {callback.total_tokens}<br>
                í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ëœ í† í° ìˆ˜: {callback.prompt_tokens}<br>
                ë‹µë³€ì— ì‚¬ìš©ëœ í† í° ìˆ˜: {callback.completion_tokens}<br>
                í˜¸ì¶œì— ì²­êµ¬ëœ ê¸ˆì•¡(USD): ${callback.total_cost:.6f}<br>
                í˜¸ì¶œì— ì²­êµ¬ëœ ê¸ˆì•¡(KRW): â‚©{callback.total_cost * 1300:.2f} (1 USD = 1300 KRW ê°€ì •)
                </div>
                """, unsafe_allow_html=True)

                # ê°œë³„ ë‰´ìŠ¤ í‘œì‹œ
                st.markdown("<p class='big-font'>í¬ë¡¤ë§ëœ ë‰´ìŠ¤</p>", unsafe_allow_html=True)
                for news in news_data:
                    with st.expander(f"{news[0]} - {news[2]}"):
                        st.write(news[1])
            else:
                st.error("ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ëª©ëª… ë˜ëŠ” ì¢…ëª© ì½”ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()